{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tujuan notebook ini adalah untuk membersihkan dan mengubah data mentah menjadi format yang bersih, terstruktur, dan siap untuk diumpankan ke model AI."
      ],
      "metadata": {
        "id": "8qSJFUEiwKsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup dan Import Library**"
      ],
      "metadata": {
        "id": "aOXLZfx8XANN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(\"üîß Starting Data Preprocessing Phase...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydW5aUaWwQFE",
        "outputId": "72c5e534-cebc-4bf0-bd6a-85badf97d4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully!\n",
            "üîß Starting Data Preprocessing Phase...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "5TYcwA4DhjID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"üìÅ Please upload your AI Assistant Usage dataset (same file as Notebook 1):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(filename)\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
        "\n",
        "# Display basic info\n",
        "print(\"\\nüìã Dataset Overview:\")\n",
        "print(df.head())\n",
        "print(f\"\\nColumns: {list(df.columns)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "W2grZo_PhmSU",
        "outputId": "335c3032-2b61-4826-ab9e-50a5495aa493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Please upload your AI Assistant Usage dataset (same file as Notebook 1):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8dc6528a-785d-4e44-84ad-cfa3cc52acd7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8dc6528a-785d-4e44-84ad-cfa3cc52acd7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ai_assistant_usage_student_life.csv to ai_assistant_usage_student_life.csv\n",
            "‚úÖ Dataset loaded: 10000 rows √ó 11 columns\n",
            "\n",
            "üìã Dataset Overview:\n",
            "      SessionID   StudentLevel        Discipline SessionDate  \\\n",
            "0  SESSION00001  Undergraduate  Computer Science  2024-11-03   \n",
            "1  SESSION00002  Undergraduate        Psychology  2024-08-25   \n",
            "2  SESSION00003  Undergraduate          Business  2025-01-12   \n",
            "3  SESSION00004  Undergraduate  Computer Science  2025-05-06   \n",
            "4  SESSION00005  Undergraduate        Psychology  2025-03-18   \n",
            "\n",
            "   SessionLengthMin  TotalPrompts  TaskType  AI_AssistanceLevel  \\\n",
            "0             31.20            11  Studying                   2   \n",
            "1             13.09             6  Studying                   3   \n",
            "2             19.22             5    Coding                   3   \n",
            "3              3.70             1    Coding                   3   \n",
            "4             28.12             9   Writing                   3   \n",
            "\n",
            "           FinalOutcome  UsedAgain  SatisfactionRating  \n",
            "0  Assignment Completed       True                 1.0  \n",
            "1  Assignment Completed       True                 2.0  \n",
            "2  Assignment Completed       True                 3.3  \n",
            "3  Assignment Completed       True                 3.5  \n",
            "4  Assignment Completed       True                 2.9  \n",
            "\n",
            "Columns: ['SessionID', 'StudentLevel', 'Discipline', 'SessionDate', 'SessionLengthMin', 'TotalPrompts', 'TaskType', 'AI_AssistanceLevel', 'FinalOutcome', 'UsedAgain', 'SatisfactionRating']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning**"
      ],
      "metadata": {
        "id": "D5Fru6b6hup_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode ini melakukan proses **pembersihan data** yang mencakup tiga tahap utama:\n",
        "\n",
        "1. **Menangani nilai hilang**\n",
        "\n",
        "   * Mengidentifikasi kolom yang memiliki missing values.\n",
        "   * Mengisi nilai hilang berdasarkan tipe data dan persentasenya:\n",
        "\n",
        "     * Data kategorikal/teks diisi dengan *mode* atau `\"Unknown\"`.\n",
        "     * Data numerik diisi dengan *median*.\n",
        "     * Kolom dengan terlalu banyak nilai hilang diberi catatan untuk dipertimbangkan dihapus.\n",
        "\n",
        "2. **Menghapus duplikat**\n",
        "\n",
        "   * Menghitung jumlah baris duplikat sebelum dan sesudah pembersihan.\n",
        "   * Menghapus baris duplikat jika ditemukan.\n",
        "\n",
        "3. **Optimisasi tipe data**\n",
        "\n",
        "   * Mengubah kolom bertipe *object* menjadi *category* jika proporsi nilai uniknya kecil (<10%).\n",
        "\n",
        "Hasil akhir menampilkan bentuk dataset setelah pembersihan, termasuk perubahan jumlah baris, kolom, dan distribusi tipe data.\n"
      ],
      "metadata": {
        "id": "Hxxnx5VVn7S-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üßπ DATA CLEANING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Original dataset info\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "\n",
        "# 1. Handle missing values\n",
        "print(\"\\n‚ùì Missing Values Handling:\")\n",
        "missing_before = df.isnull().sum().sum()\n",
        "print(f\"Missing values before cleaning: {missing_before}\")\n",
        "\n",
        "# Identify missing patterns\n",
        "missing_data = df.isnull().sum()\n",
        "missing_cols = missing_data[missing_data > 0].index.tolist()\n",
        "\n",
        "if missing_cols:\n",
        "    print(f\"Columns with missing values: {missing_cols}\")\n",
        "\n",
        "    for col in missing_cols:\n",
        "        missing_count = df[col].isnull().sum()\n",
        "        missing_percent = (missing_count / len(df)) * 100\n",
        "        print(f\"  - {col}: {missing_count} ({missing_percent:.2f}%)\")\n",
        "\n",
        "        if df[col].dtype == 'object':\n",
        "            # For categorical/text data, fill with 'Unknown' or mode\n",
        "            if missing_percent < 10:\n",
        "                mode_val = df[col].mode()\n",
        "                if len(mode_val) > 0:\n",
        "                    df[col].fillna(mode_val[0], inplace=True)\n",
        "                    print(f\"    ‚Üí Filled with mode: {mode_val[0]}\")\n",
        "                else:\n",
        "                    df[col].fillna('Unknown', inplace=True)\n",
        "                    print(f\"    ‚Üí Filled with 'Unknown'\")\n",
        "            else:\n",
        "                df[col].fillna('Unknown', inplace=True)\n",
        "                print(f\"    ‚Üí Filled with 'Unknown'\")\n",
        "        else:\n",
        "            # For numerical data, fill with median\n",
        "            if missing_percent < 10:\n",
        "                median_val = df[col].median()\n",
        "                df[col].fillna(median_val, inplace=True)\n",
        "                print(f\"    ‚Üí Filled with median: {median_val}\")\n",
        "            else:\n",
        "                # If too many missing, consider dropping the column\n",
        "                print(f\"    ‚Üí Too many missing values ({missing_percent:.2f}%), consider dropping\")\n",
        "\n",
        "missing_after = df.isnull().sum().sum()\n",
        "print(f\"\\nMissing values after cleaning: {missing_after}\")\n",
        "\n",
        "# 2. Handle duplicates\n",
        "print(\"\\nüîÑ Duplicate Handling:\")\n",
        "duplicates_before = df.duplicated().sum()\n",
        "print(f\"Duplicates before: {duplicates_before}\")\n",
        "\n",
        "if duplicates_before > 0:\n",
        "    df = df.drop_duplicates()\n",
        "    duplicates_after = df.duplicated().sum()\n",
        "    print(f\"Duplicates after: {duplicates_after}\")\n",
        "    print(f\"Rows removed: {duplicates_before}\")\n",
        "\n",
        "# 3. Data type optimization\n",
        "print(\"\\nüìä Data Type Optimization:\")\n",
        "print(\"Before optimization:\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "# Convert appropriate columns to categorical\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        unique_ratio = df[col].nunique() / len(df)\n",
        "        if unique_ratio < 0.1:  # If less than 10% unique values, make categorical\n",
        "            df[col] = df[col].astype('category')\n",
        "            print(f\"  - {col} converted to category ({df[col].nunique()} unique values)\")\n",
        "\n",
        "print(\"\\nAfter optimization:\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset shape after cleaning: {df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "logJR7Elht_t",
        "outputId": "07c8973c-99b2-4c77-8231-a78ee6ae3cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üßπ DATA CLEANING\n",
            "============================================================\n",
            "Original dataset shape: (10000, 11)\n",
            "\n",
            "‚ùì Missing Values Handling:\n",
            "Missing values before cleaning: 0\n",
            "\n",
            "Missing values after cleaning: 0\n",
            "\n",
            "üîÑ Duplicate Handling:\n",
            "Duplicates before: 0\n",
            "\n",
            "üìä Data Type Optimization:\n",
            "Before optimization:\n",
            "float64     2\n",
            "int64       2\n",
            "object      1\n",
            "category    1\n",
            "category    1\n",
            "category    1\n",
            "category    1\n",
            "category    1\n",
            "bool        1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After optimization:\n",
            "float64     2\n",
            "int64       2\n",
            "object      1\n",
            "category    1\n",
            "category    1\n",
            "category    1\n",
            "category    1\n",
            "category    1\n",
            "bool        1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "‚úÖ Dataset shape after cleaning: (10000, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing**"
      ],
      "metadata": {
        "id": "M7BeV_kbh6EI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode ini menjalankan **text preprocessing** secara menyeluruh dengan langkah-langkah berikut:\n",
        "\n",
        "1. **Identifikasi kolom teks**\n",
        "\n",
        "   * Mencari kolom bertipe *object* atau *category* yang berisi teks panjang (>50 karakter) atau memiliki kata kunci seperti `\"comment\"`, `\"feedback\"`, `\"description\"`, `\"text\"`, atau `\"review\"`.\n",
        "\n",
        "2. **Persiapan alat preprocessing**\n",
        "\n",
        "   * Menggunakan **WordNetLemmatizer** untuk normalisasi kata.\n",
        "   * Menghapus **stopwords** bahasa Inggris.\n",
        "\n",
        "3. **Fungsi `preprocess_text`** (proses pembersihan teks):\n",
        "\n",
        "   * Mengubah teks menjadi huruf kecil.\n",
        "   * Menghapus URL, email, tag HTML, karakter khusus, angka, dan spasi berlebih.\n",
        "   * Tokenisasi teks menjadi kata-kata.\n",
        "   * Menghapus stopwords dan kata pendek (<3 huruf).\n",
        "   * Melakukan **lemmatization** agar kata kembali ke bentuk dasarnya.\n",
        "\n",
        "4. **Penerapan preprocessing**\n",
        "\n",
        "   * Memproses setiap kolom teks menjadi kolom baru dengan akhiran `_processed`.\n",
        "   * Menampilkan contoh sebelum dan sesudah preprocessing.\n",
        "   * Menghitung rata-rata panjang teks sebelum dan sesudah pembersihan.\n",
        "\n",
        "Hasilnya adalah dataset dengan versi teks yang sudah bersih, terstruktur, dan siap untuk analisis lanjutan atau pemodelan NLP.\n"
      ],
      "metadata": {
        "id": "xMdc1ypioFSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìù TEXT PREPROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Identify text columns\n",
        "text_columns = []\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in ['object', 'category']:\n",
        "        # Check if it's likely a text column\n",
        "        sample_text = df[col].dropna().astype(str).iloc[0] if not df[col].dropna().empty else \"\"\n",
        "        if len(sample_text) > 50 or any(keyword in col.lower() for keyword in ['comment', 'feedback', 'description', 'text', 'review']):\n",
        "            text_columns.append(col)\n",
        "\n",
        "print(f\"Text columns identified: {text_columns}\")\n",
        "\n",
        "# Initialize text preprocessing tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Comprehensive text preprocessing function\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to string and lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and short words\n",
        "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
        "\n",
        "    # Lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply text preprocessing\n",
        "for col in text_columns:\n",
        "    print(f\"\\nüîß Preprocessing text column: {col}\")\n",
        "\n",
        "    # Show sample before preprocessing\n",
        "    print(\"Sample before preprocessing:\")\n",
        "    sample_idx = df[col].dropna().index[0] if not df[col].dropna().empty else None\n",
        "    if sample_idx is not None:\n",
        "        print(f\"  Original: {df.loc[sample_idx, col][:200]}...\")\n",
        "\n",
        "    # Create preprocessed version\n",
        "    preprocessed_col = f\"{col}_processed\"\n",
        "    df[preprocessed_col] = df[col].apply(preprocess_text)\n",
        "\n",
        "    # Show sample after preprocessing\n",
        "    if sample_idx is not None:\n",
        "        print(f\"  Processed: {df.loc[sample_idx, preprocessed_col][:200]}...\")\n",
        "\n",
        "    # Basic statistics\n",
        "    avg_length_before = df[col].dropna().astype(str).str.len().mean()\n",
        "    avg_length_after = df[preprocessed_col].str.len().mean()\n",
        "    print(f\"  Average length before: {avg_length_before:.1f} characters\")\n",
        "    print(f\"  Average length after: {avg_length_after:.1f} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cN6v0cUh9JF",
        "outputId": "e04ab38f-98ee-488d-935e-7164e56e3352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìù TEXT PREPROCESSING\n",
            "============================================================\n",
            "Text columns identified: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**"
      ],
      "metadata": {
        "id": "JtQA4RBJiCCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode ini menjalankan **feature engineering** dengan beberapa tahap utama:\n",
        "\n",
        "1. **Encoding variabel kategorikal**\n",
        "\n",
        "   * Mengambil kolom kategorikal (`object`/`category`) yang bukan kolom teks.\n",
        "   * Jika jumlah kategori ‚â§ 10 ‚Üí **One-hot encoding**.\n",
        "   * Jika jumlah kategori > 10 ‚Üí **Label encoding**.\n",
        "\n",
        "2. **Scaling variabel numerik**\n",
        "\n",
        "   * Mengambil semua kolom numerik (`int64`, `float64`).\n",
        "   * Menggunakan **StandardScaler** untuk standarisasi (mean=0, std=1).\n",
        "\n",
        "3. **Ekstraksi fitur teks**\n",
        "\n",
        "   * Menggunakan versi teks yang sudah diproses (`_processed`).\n",
        "   * Membuat fitur **TF-IDF** (max 100 fitur, unigram + bigram, batas frekuensi min 2 dokumen, max 80%).\n",
        "\n",
        "4. **Membuat matriks fitur akhir**\n",
        "\n",
        "   * Menggabungkan semua fitur hasil encoding, scaling, dan ekstraksi teks menjadi satu DataFrame (`X_processed`).\n",
        "   * Menampilkan jumlah fitur akhir (baris √ó kolom).\n",
        "\n",
        "Hasil akhirnya adalah **dataset terstandardisasi dan terenkode** yang siap digunakan untuk pemodelan machine learning.\n"
      ],
      "metadata": {
        "id": "KCfo2fvAooXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚öôÔ∏è FEATURE ENGINEERING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Categorical Encoding\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "# Remove text columns from categorical encoding\n",
        "categorical_cols = [col for col in categorical_cols if col not in text_columns]\n",
        "\n",
        "print(f\"\\nüè∑Ô∏è Categorical columns for encoding: {categorical_cols}\")\n",
        "\n",
        "# Create encoded features\n",
        "encoded_features = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\nEncoding {col}:\")\n",
        "    unique_values = df[col].nunique()\n",
        "    print(f\"  Unique values: {unique_values}\")\n",
        "\n",
        "    if unique_values <= 10:\n",
        "        # One-hot encoding for low cardinality\n",
        "        encoded = pd.get_dummies(df[col], prefix=col)\n",
        "        encoded_features[col] = encoded\n",
        "        print(f\"  ‚Üí One-hot encoded ({encoded.shape[1]} features)\")\n",
        "    else:\n",
        "        # Label encoding for high cardinality\n",
        "        le = LabelEncoder()\n",
        "        encoded = le.fit_transform(df[col].astype(str))\n",
        "        encoded_df = pd.DataFrame({f\"{col}_encoded\": encoded})\n",
        "        encoded_features[col] = encoded_df\n",
        "        print(f\"  ‚Üí Label encoded (1 feature)\")\n",
        "\n",
        "# 2. Numerical Feature Scaling\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "print(f\"\\nüî¢ Numerical columns for scaling: {numerical_cols}\")\n",
        "\n",
        "scaled_features = {}\n",
        "if numerical_cols:\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df[numerical_cols])\n",
        "    scaled_features = pd.DataFrame(\n",
        "        scaled_data,\n",
        "        columns=[f\"{col}_scaled\" for col in numerical_cols],\n",
        "        index=df.index\n",
        "    )\n",
        "    print(f\"  ‚Üí {len(numerical_cols)} numerical features scaled\")\n",
        "\n",
        "# 3. Text Feature Extraction\n",
        "text_features = {}\n",
        "for col in text_columns:\n",
        "    processed_col = f\"{col}_processed\"\n",
        "    if processed_col in df.columns:\n",
        "        print(f\"\\nüìÑ Text feature extraction for {col}:\")\n",
        "\n",
        "        # TF-IDF Features\n",
        "        tfidf = TfidfVectorizer(\n",
        "            max_features=100,  # Limit features for demonstration\n",
        "            ngram_range=(1, 2),  # Unigrams and bigrams\n",
        "            min_df=2,  # Ignore terms that appear in less than 2 documents\n",
        "            max_df=0.8  # Ignore terms that appear in more than 80% of documents\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            tfidf_features = tfidf.fit_transform(df[processed_col])\n",
        "            feature_names = [f\"{col}_tfidf_{name}\" for name in tfidf.get_feature_names_out()]\n",
        "            tfidf_df = pd.DataFrame(\n",
        "                tfidf_features.toarray(),\n",
        "                columns=feature_names,\n",
        "                index=df.index\n",
        "            )\n",
        "            text_features[f\"{col}_tfidf\"] = tfidf_df\n",
        "            print(f\"  ‚Üí TF-IDF features: {tfidf_df.shape[1]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è TF-IDF failed: {str(e)}\")\n",
        "\n",
        "# 4. Create Final Feature Matrix\n",
        "print(\"\\nüî® Creating Final Feature Matrix:\")\n",
        "\n",
        "# Combine all features\n",
        "feature_dfs = []\n",
        "\n",
        "# Add encoded categorical features\n",
        "for col, features in encoded_features.items():\n",
        "    feature_dfs.append(features)\n",
        "    print(f\"  + {col}: {features.shape[1]} features\")\n",
        "\n",
        "# Add scaled numerical features\n",
        "if len(scaled_features) > 0:\n",
        "    feature_dfs.append(scaled_features)\n",
        "    print(f\"  + Numerical (scaled): {scaled_features.shape[1]} features\")\n",
        "\n",
        "# Add text features\n",
        "for col, features in text_features.items():\n",
        "    feature_dfs.append(features)\n",
        "    print(f\"  + {col}: {features.shape[1]} features\")\n",
        "\n",
        "# Combine all features\n",
        "if feature_dfs:\n",
        "    X_processed = pd.concat(feature_dfs, axis=1)\n",
        "    print(f\"\\n‚úÖ Final feature matrix: {X_processed.shape[0]} rows √ó {X_processed.shape[1]} features\")\n",
        "else:\n",
        "    X_processed = df[numerical_cols] if numerical_cols else pd.DataFrame()\n",
        "    print(f\"\\n‚ö†Ô∏è Using original numerical features: {X_processed.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9mnIYJyiGZr",
        "outputId": "f941a280-3c31-4ce8-f14d-59f8b85fa65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "‚öôÔ∏è FEATURE ENGINEERING\n",
            "============================================================\n",
            "\n",
            "üè∑Ô∏è Categorical columns for encoding: ['SessionID', 'StudentLevel', 'Discipline', 'SessionDate', 'TaskType', 'FinalOutcome']\n",
            "\n",
            "Encoding SessionID:\n",
            "  Unique values: 10000\n",
            "  ‚Üí Label encoded (1 feature)\n",
            "\n",
            "Encoding StudentLevel:\n",
            "  Unique values: 3\n",
            "  ‚Üí One-hot encoded (3 features)\n",
            "\n",
            "Encoding Discipline:\n",
            "  Unique values: 7\n",
            "  ‚Üí One-hot encoded (7 features)\n",
            "\n",
            "Encoding SessionDate:\n",
            "  Unique values: 366\n",
            "  ‚Üí Label encoded (1 feature)\n",
            "\n",
            "Encoding TaskType:\n",
            "  Unique values: 6\n",
            "  ‚Üí One-hot encoded (6 features)\n",
            "\n",
            "Encoding FinalOutcome:\n",
            "  Unique values: 4\n",
            "  ‚Üí One-hot encoded (4 features)\n",
            "\n",
            "üî¢ Numerical columns for scaling: ['SessionLengthMin', 'TotalPrompts', 'AI_AssistanceLevel', 'SatisfactionRating']\n",
            "  ‚Üí 4 numerical features scaled\n",
            "\n",
            "üî® Creating Final Feature Matrix:\n",
            "  + SessionID: 1 features\n",
            "  + StudentLevel: 3 features\n",
            "  + Discipline: 7 features\n",
            "  + SessionDate: 1 features\n",
            "  + TaskType: 6 features\n",
            "  + FinalOutcome: 4 features\n",
            "  + Numerical (scaled): 4 features\n",
            "\n",
            "‚úÖ Final feature matrix: 10000 rows √ó 26 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Target Variable Prep**"
      ],
      "metadata": {
        "id": "ZJS5j9bliULT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode ini menyiapkan **kolom target** untuk pemodelan dengan langkah-langkah yang sederhana:\n",
        "\n",
        "1. **Mencari kolom yang cocok jadi target**\n",
        "\n",
        "   * Mencari kolom yang namanya mengandung kata seperti *category*, *type*, *class*, *usage*, *purpose*, atau *label*.\n",
        "   * Kalau tidak ada, mencari kolom kategorikal yang jumlah kategorinya antara 2‚Äì10.\n",
        "\n",
        "2. **Menampilkan kandidat target**\n",
        "\n",
        "   * Menunjukkan daftar kolom yang bisa dijadikan target prediksi.\n",
        "\n",
        "3. **Mengubah target jadi angka (encoding)**\n",
        "\n",
        "   * Menggunakan **LabelEncoder** untuk mengganti setiap kategori dengan angka unik.\n",
        "   * Menyimpan encoder ini supaya bisa digunakan lagi nanti.\n",
        "\n",
        "Hasil akhirnya: kolom target sudah dalam bentuk angka sehingga siap dipakai untuk algoritma machine learning.\n"
      ],
      "metadata": {
        "id": "moCS41PDow7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ TARGET VARIABLE PREPARATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Identify potential target variables\n",
        "potential_targets = []\n",
        "for col in df.columns:\n",
        "    if any(keyword in col.lower() for keyword in ['category', 'type', 'class', 'usage', 'purpose', 'label']):\n",
        "        potential_targets.append(col)\n",
        "\n",
        "if not potential_targets:\n",
        "    # If no obvious target, look for categorical columns with reasonable number of classes\n",
        "    for col in categorical_cols:\n",
        "        unique_count = df[col].nunique()\n",
        "        if 2 <= unique_count <= 10:\n",
        "            potential_targets.append(col)\n",
        "\n",
        "print(f\"Potential target variables: {potential_targets}\")\n",
        "\n",
        "# Prepare target variables\n",
        "target_encoders = {}\n",
        "y_encoded = {}\n",
        "\n",
        "for target_col in potential_targets:\n",
        "    print(f\"\\nüéØ Preparing target: {target_col}\")\n",
        "    print(f\"  Classes: {df[target_col].value_counts().to_dict()}\")\n",
        "\n",
        "    # Encode target variable\n",
        "    le = LabelEncoder()\n",
        "    y_encoded[target_col] = le.fit_transform(df[target_col].astype(str))\n",
        "    target_encoders[target_col] = le\n",
        "\n",
        "    print(f\"  Encoded classes: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spJKXSDFiXC8",
        "outputId": "ac9937a2-2ec5-421c-883b-17e8679528c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üéØ TARGET VARIABLE PREPARATION\n",
            "============================================================\n",
            "Potential target variables: ['TaskType']\n",
            "\n",
            "üéØ Preparing target: TaskType\n",
            "  Classes: {'Writing': 3101, 'Studying': 2040, 'Homework Help': 1959, 'Coding': 1948, 'Brainstorming': 476, 'Research': 476}\n",
            "  Encoded classes: {'Brainstorming': np.int64(0), 'Coding': np.int64(1), 'Homework Help': np.int64(2), 'Research': np.int64(3), 'Studying': np.int64(4), 'Writing': np.int64(5)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Test Split**"
      ],
      "metadata": {
        "id": "HEVvE765kRLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode ini membagi data menjadi **data latih** dan **data uji** dengan langkah-langkah berikut:\n",
        "\n",
        "1. **Menentukan target utama**\n",
        "\n",
        "   * Mengambil target pertama dari daftar kandidat yang sudah ditemukan sebelumnya untuk dipakai dalam pemisahan data.\n",
        "\n",
        "2. **Membagi data (train-test split)**\n",
        "\n",
        "   * Memisahkan fitur (`X_processed`) dan target (`y_encoded`) menjadi:\n",
        "\n",
        "     * **Data latih (80%)** ‚Äì untuk melatih model.\n",
        "     * **Data uji (20%)** ‚Äì untuk menguji performa model.\n",
        "   * Menggunakan parameter `stratify` agar proporsi tiap kelas di train dan test tetap seimbang.\n",
        "\n",
        "3. **Menampilkan informasi pembagian data**\n",
        "\n",
        "   * Jumlah sampel di data latih dan data uji.\n",
        "   * Jumlah fitur yang digunakan.\n",
        "   * Distribusi kelas target pada kedua subset.\n",
        "\n",
        "4. **Jika tidak ada target**\n",
        "\n",
        "   * Tetap membagi fitur menjadi train dan test, tapi tanpa target variabel.\n"
      ],
      "metadata": {
        "id": "0NOIGwBJpMNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if len(potential_targets) > 0:\n",
        "    # Use first target for splitting\n",
        "    main_target = potential_targets[0]\n",
        "    print(f\"Using {main_target} as main target for splitting\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_processed,\n",
        "        y_encoded[main_target],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y_encoded[main_target]\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "    print(f\"Features: {X_train.shape[1]}\")\n",
        "\n",
        "    # Show target distribution\n",
        "    train_dist = pd.Series(y_train).value_counts().sort_index()\n",
        "    test_dist = pd.Series(y_test).value_counts().sort_index()\n",
        "\n",
        "    print(\"\\nTarget distribution:\")\n",
        "    print(\"Train:\", train_dist.to_dict())\n",
        "    print(\"Test: \", test_dist.to_dict())\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No suitable target variable found for classification\")\n",
        "    X_train, X_test = train_test_split(X_processed, test_size=0.2, random_state=42)\n",
        "    y_train = y_test = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3sjtoGPkT7L",
        "outputId": "390922db-7e6e-4ea3-c55f-ccd7a531955d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìä TRAIN-TEST SPLIT\n",
            "============================================================\n",
            "Using TaskType as main target for splitting\n",
            "Training set: 8000 samples\n",
            "Test set: 2000 samples\n",
            "Features: 26\n",
            "\n",
            "Target distribution:\n",
            "Train: {0: 381, 1: 1558, 2: 1567, 3: 381, 4: 1632, 5: 2481}\n",
            "Test:  {0: 95, 1: 390, 2: 392, 3: 95, 4: 408, 5: 620}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Validation**"
      ],
      "metadata": {
        "id": "NvIzs1T7keBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode ini melakukan **validasi akhir data** sebelum digunakan untuk pemodelan, dengan langkah-langkah:\n",
        "\n",
        "1. **Pemeriksaan nilai tak hingga (infinite values)**\n",
        "\n",
        "   * Mengecek apakah ada nilai `‚àû` atau `-‚àû` di fitur numerik.\n",
        "   * Jika ada, nilainya diganti menjadi `NaN` lalu diisi dengan `0`.\n",
        "\n",
        "2. **Pemeriksaan nilai kosong (NaN)**\n",
        "\n",
        "   * Mengecek apakah ada nilai `NaN` di data yang sudah diproses.\n",
        "   * Jika ditemukan, semua `NaN` diisi dengan `0`.\n",
        "\n",
        "3. **Ringkasan statistik fitur**\n",
        "\n",
        "   * Menampilkan ukuran matriks fitur (`shape`).\n",
        "   * Menghitung penggunaan memori.\n",
        "   * Menyajikan jumlah masing-masing tipe data yang digunakan.\n"
      ],
      "metadata": {
        "id": "BvXZ_aLOpYrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ DATA VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check for any remaining issues\n",
        "print(\"Final data validation:\")\n",
        "\n",
        "# 1. Check for infinite values\n",
        "if X_processed.select_dtypes(include=[np.number]).isin([np.inf, -np.inf]).any().any():\n",
        "    print(\"‚ö†Ô∏è Infinite values found in features\")\n",
        "    # Replace infinite values\n",
        "    X_processed = X_processed.replace([np.inf, -np.inf], np.nan)\n",
        "    X_processed = X_processed.fillna(0)\n",
        "    print(\"  ‚Üí Infinite values replaced with 0\")\n",
        "else:\n",
        "    print(\"‚úÖ No infinite values in features\")\n",
        "\n",
        "# 2. Check for NaN values\n",
        "if X_processed.isnull().any().any():\n",
        "    print(\"‚ö†Ô∏è NaN values found in processed features\")\n",
        "    nan_count = X_processed.isnull().sum().sum()\n",
        "    print(f\"  ‚Üí {nan_count} NaN values found and filled with 0\")\n",
        "    X_processed = X_processed.fillna(0)\n",
        "else:\n",
        "    print(\"‚úÖ No NaN values in processed features\")\n",
        "\n",
        "# 3. Feature statistics\n",
        "print(f\"\\nFeature matrix statistics:\")\n",
        "print(f\"  Shape: {X_processed.shape}\")\n",
        "print(f\"  Memory usage: {X_processed.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"  Data types: {X_processed.dtypes.value_counts().to_dict()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYk_hBSfkrHx",
        "outputId": "6f4417a8-9bc2-4f6b-ebd7-e3fb8ae7f895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "‚úÖ DATA VALIDATION\n",
            "============================================================\n",
            "Final data validation:\n",
            "‚úÖ No infinite values in features\n",
            "‚úÖ No NaN values in processed features\n",
            "\n",
            "Feature matrix statistics:\n",
            "  Shape: (10000, 26)\n",
            "  Memory usage: 0.65 MB\n",
            "  Data types: {dtype('bool'): 20, dtype('float64'): 4, dtype('int64'): 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save Processed Data**"
      ],
      "metadata": {
        "id": "PUIQWgaIkwu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode ini menyimpan dan merangkum hasil akhir preprocessing data\n",
        "\n",
        "* **Membuat ringkasan preprocessing** yang berisi:\n",
        "\n",
        "  * **Bentuk data asli dan akhir** (jumlah baris √ó kolom sebelum dan sesudah pemrosesan).\n",
        "  * **Kolom teks, kategorikal, dan numerikal** yang digunakan.\n",
        "  * **Variabel target** yang teridentifikasi.\n",
        "  * **Jumlah fitur baru** yang dihasilkan dari encoding kategorikal, scaling numerikal, dan ekstraksi teks, beserta total fitur akhir.\n",
        "\n",
        "* **Menampilkan status akhir**:\n",
        "\n",
        "  * Konfirmasi bahwa data siap untuk modeling, lengkap dengan jumlah fitur yang tersedia.\n",
        "  * Menunjukkan target klasifikasi yang siap digunakan, jika ada.\n",
        "  * Menyebutkan data teks yang siap dipakai untuk summarization.\n",
        "\n",
        "* **Memberikan panduan langkah selanjutnya**, yaitu:\n",
        "\n",
        "  1. Analisis klasifikasi.\n",
        "  2. Ringkasan teks.\n",
        "  3. Evaluasi model.\n",
        "  4. Pembuatan dashboard visualisasi.\n",
        "\n",
        "* **Informasi matriks fitur terakhir**:\n",
        "\n",
        "  * Jumlah sampel dan fitur.\n",
        "  * Nama variabel target utama beserta jumlah kelasnya.\n"
      ],
      "metadata": {
        "id": "T9D6hG_8pqHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üíæ SAVE PROCESSED DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create summary of preprocessing steps\n",
        "preprocessing_summary = {\n",
        "    'original_shape': df.shape,\n",
        "    'final_shape': X_processed.shape,\n",
        "    'text_columns': text_columns,\n",
        "    'categorical_columns': categorical_cols,\n",
        "    'numerical_columns': numerical_cols,\n",
        "    'target_variables': potential_targets,\n",
        "    'features_created': {\n",
        "        'encoded_categorical': sum(df.shape[1] for df in encoded_features.values()),\n",
        "        'scaled_numerical': scaled_features.shape[1] if len(scaled_features) > 0 else 0,\n",
        "        'text_features': sum(df.shape[1] for df in text_features.values()),\n",
        "        'total_features': X_processed.shape[1]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Preprocessing Summary:\")\n",
        "for key, value in preprocessing_summary.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Save data for next notebooks\n",
        "print(f\"\\n‚úÖ Data preprocessing completed!\")\n",
        "print(f\"üìä Ready for modeling with {X_processed.shape[1]} features\")\n",
        "\n",
        "if potential_targets:\n",
        "    print(f\"üéØ Classification targets available: {potential_targets}\")\n",
        "if text_columns:\n",
        "    print(f\"üìù Text data ready for summarization: {text_columns}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ NEXT STEPS\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Classification Analysis (Notebook 3)\")\n",
        "print(\"2. Text Summarization (Notebook 4)\")\n",
        "print(\"3. Model Evaluation (Notebook 5)\")\n",
        "print(\"4. Visualization Dashboard (Notebook 6)\")\n",
        "\n",
        "# Display final feature info\n",
        "print(f\"\\nüìã Feature Matrix Ready: {X_processed.shape[0]} samples √ó {X_processed.shape[1]} features\")\n",
        "if len(potential_targets) > 0:\n",
        "    print(f\"üéØ Target Variable Ready: {main_target} ({len(np.unique(y_encoded[main_target]))} classes)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvbEmYPtkzUI",
        "outputId": "1183a339-27b2-4fb4-ca7d-3e56e987f521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üíæ SAVE PROCESSED DATA\n",
            "============================================================\n",
            "Preprocessing Summary:\n",
            "  original_shape: (10000, 11)\n",
            "  final_shape: (10000, 26)\n",
            "  text_columns: []\n",
            "  categorical_columns: ['SessionID', 'StudentLevel', 'Discipline', 'SessionDate', 'TaskType', 'FinalOutcome']\n",
            "  numerical_columns: ['SessionLengthMin', 'TotalPrompts', 'AI_AssistanceLevel', 'SatisfactionRating']\n",
            "  target_variables: ['TaskType']\n",
            "  features_created: {'encoded_categorical': 22, 'scaled_numerical': 4, 'text_features': 0, 'total_features': 26}\n",
            "\n",
            "‚úÖ Data preprocessing completed!\n",
            "üìä Ready for modeling with 26 features\n",
            "üéØ Classification targets available: ['TaskType']\n",
            "\n",
            "============================================================\n",
            "üöÄ NEXT STEPS\n",
            "============================================================\n",
            "1. Classification Analysis (Notebook 3)\n",
            "2. Text Summarization (Notebook 4)\n",
            "3. Model Evaluation (Notebook 5)\n",
            "4. Visualization Dashboard (Notebook 6)\n",
            "\n",
            "üìã Feature Matrix Ready: 10000 samples √ó 26 features\n",
            "üéØ Target Variable Ready: TaskType (6 classes)\n"
          ]
        }
      ]
    }
  ]
}